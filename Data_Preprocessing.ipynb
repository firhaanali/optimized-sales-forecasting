{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGJnG8sgvITLTAws1DCvlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firhaanali/optimized-sales-forecasting/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install requests\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "HqMK3WKinX3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "PiI4Ggtjx8M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import xgboost as xgb\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive and load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Dataset/dataset_merged.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "-uEFJr9VmvZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n"
      ],
      "metadata": {
        "id": "OrJvUDkFyERq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standarisasi nama kolom\n",
        "original_columns = df.columns.tolist()\n",
        "df.columns = df.columns.str.strip().str.replace(\" \", \"_\", regex=False)\n",
        "\n",
        "# Check missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(len(df.columns)), [1]*len(df.columns))  # Membuat semua batang dengan tinggi 1\n",
        "plt.yticks(range(len(df.columns)), df.columns)\n",
        "plt.title('Daftar Fitur dalam Dataset')\n",
        "plt.xlabel('Jumlah')\n",
        "plt.ylabel('Fitur (Kolom)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lU6iwb2HnQiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "df['Size'] = df['Size'].fillna('All_Size').astype(str).str.replace(\"Ld \", \"\", case=False).str.strip()\n",
        "df['Payment_platform_discount'] = pd.to_numeric(df['Payment_platform_discount'], errors='coerce').fillna(0)\n",
        "df['Handling_Fee'] = pd.to_numeric(df['Handling_Fee'], errors='coerce').fillna(0)"
      ],
      "metadata": {
        "id": "zYEbDNcfJ1WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "gwyQ01CWyKbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Extraction"
      ],
      "metadata": {
        "id": "9Jp4ed35yNmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_columns = df.columns.tolist()\n",
        "\n",
        "# Feature Engineering - Time Features\n",
        "if 'Created_Time' in df.columns:\n",
        "    df['Created_Time'] = pd.to_datetime(df['Created_Time'], errors='coerce', dayfirst=True)\n",
        "    df.dropna(subset=['Created_Time'], inplace=True)\n",
        "\n",
        "# Extract basic time features\n",
        "df['year'] = df['Created_Time'].dt.year\n",
        "df['month'] = df['Created_Time'].dt.month\n",
        "df['day'] = df['Created_Time'].dt.day\n",
        "df['hour'] = df['Created_Time'].dt.hour\n",
        "df['minute'] = df['Created_Time'].dt.minute\n",
        "df['second'] = df['Created_Time'].dt.second\n",
        "df['day_of_week'] = df['Created_Time'].dt.dayofweek\n",
        "df['day_of_year'] = df['Created_Time'].dt.dayofyear\n",
        "df['quarter'] = df['Created_Time'].dt.quarter\n",
        "df['week_of_year'] = df['Created_Time'].dt.isocalendar().week\n",
        "\n",
        "# Cyclical transformations\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
        "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "# Calendar/event features\n",
        "df['is_payday'] = df['day'].isin([25, 26, 27, 28, 29, 30, 31, 1, 2]).astype(int)\n",
        "df['date'] = df['Created_Time'].dt.date\n",
        "df['year_month'] = df['Created_Time'].dt.strftime('%Y-%m')\n",
        "df['year_week'] = df['year'].astype(str) + '-' + df['week_of_year'].astype(str).str.zfill(2)\n",
        "\n",
        "# Flash sale features\n",
        "flash_dates = [f\"{str(i).zfill(2)}-{str(i).zfill(2)}\" for i in range(1, 13)]\n",
        "df['flash_date_str'] = df['month'].astype(str).str.zfill(2) + '-' + df['day'].astype(str).str.zfill(2)\n",
        "df['is_flash_sale'] = df['flash_date_str'].isin(flash_dates).astype(int)\n",
        "\n",
        "# National holidays\n",
        "years = [2022, 2023, 2024, 2025]\n",
        "holiday_dates = []\n",
        "\n",
        "for year in years:\n",
        "    url = f\"https://date.nager.at/api/v3/PublicHolidays/{year}/ID\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        for holiday in data:\n",
        "            holiday_dates.append(holiday['date'])\n",
        "\n",
        "libur_nasional = pd.to_datetime(holiday_dates)\n",
        "df['is_holiday'] = df['Created_Time'].dt.date.isin(libur_nasional.date).astype(int)\n",
        "\n",
        "# Additional flags\n",
        "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "df['is_business_hour'] = (((df['hour'] >= 8) & (df['hour'] <= 17)) & (df['day_of_week'] < 5)).astype(int)\n",
        "df['is_month_start'] = (df['day'] <= 7).astype(int)\n",
        "df['is_month_end'] = (df['day'] >= 24).astype(int)\n",
        "\n",
        "after_time_columns = df.columns.tolist()\n",
        "added_time_features = [col for col in after_time_columns if col not in original_columns]\n",
        "\n",
        "# Visualisasi\n",
        "if added_time_features:\n",
        "    fig, ax = plt.subplots(figsize=(8, len(added_time_features) * 0.3))\n",
        "    bars = ax.barh(added_time_features, [1]*len(added_time_features), edgecolor='black')\n",
        "\n",
        "    for bar in bars:\n",
        "        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)\n",
        "\n",
        "    ax.set_title('Fitur Baru dari Feature Engineering Waktu', fontsize=14, pad=10)\n",
        "    ax.set_xlabel('Fitur Baru')\n",
        "    ax.set_xlim(0, 1.2)\n",
        "    ax.set_xticks([])\n",
        "    ax.invert_yaxis()\n",
        "    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)\n",
        "    plt.grid(False)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fMV1WorznOA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time - Series"
      ],
      "metadata": {
        "id": "Ed10u_bUyQzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - Time Series Features\n",
        "df = df.sort_values('Created_Time')\n",
        "df['date'] = df['Created_Time'].dt.date\n",
        "\n",
        "if 'Product_Name' in df.columns and 'Created_Time' in df.columns:\n",
        "    # Daily sales aggregation per product\n",
        "    daily_sales = df.groupby(['Product_Name', 'date'])['Quantity'].sum().reset_index()\n",
        "    daily_sales = daily_sales.sort_values(['Product_Name', 'date'])\n",
        "\n",
        "    # Create lag features\n",
        "    for lag in [1, 7, 14, 30]:\n",
        "        daily_sales[f'lag_{lag}_days'] = daily_sales.groupby('Product_Name')['Quantity'].shift(lag)\n",
        "\n",
        "    # Moving Averages & Rolling Sum\n",
        "    for window in [7, 14, 30]:\n",
        "        daily_sales[f'moving_avg_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "        )\n",
        "        daily_sales[f'rolling_sum_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).sum()\n",
        "        )\n",
        "\n",
        "    # Merge time series features to main dataframe\n",
        "    df = pd.merge(\n",
        "        df,\n",
        "        daily_sales[['Product_Name', 'date'] +\n",
        "                    [f'lag_{lag}_days' for lag in [1, 7, 14, 30]] +\n",
        "                    [f'moving_avg_{window}' for window in [7, 14, 30]] +\n",
        "                    [f'rolling_sum_{window}' for window in [7, 14, 30]]],\n",
        "        on=['Product_Name', 'date'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Volatility features\n",
        "    df['demand_std_7days'] = daily_sales.groupby('Product_Name')['Quantity'].transform(\n",
        "        lambda x: x.rolling(window=7, min_periods=1).std()\n",
        "    )\n",
        "\n",
        "    # Sales trends and ratios\n",
        "    df['sales_trend'] = (df['lag_7_days'] - df['lag_14_days']) / (df['lag_14_days'] + 1) * 100\n",
        "    df['sales_ratio_to_avg'] = df['Quantity'] / (df['moving_avg_30'] + 1)\n",
        "\n",
        "# Flash sale features\n",
        "df['flash_sale_yesterday'] = df.groupby('Product_Name')['is_flash_sale'].shift(1).fillna(0)\n",
        "df['days_since_last_flash'] = (\n",
        "    df[::-1].groupby('Product_Name')['is_flash_sale']\n",
        "    .apply(lambda x: x.cumsum().shift(-1).fillna(0))\n",
        "    .reset_index(level=0, drop=True)[::-1]\n",
        ")\n",
        "\n",
        "# Holiday and payday features\n",
        "df['holiday_yesterday'] = df['is_holiday'].shift(1).fillna(0)\n",
        "df['payday_yesterday'] = df['is_payday'].shift(1).fillna(0)\n",
        "\n",
        "# Additional time features\n",
        "df['week_of_month'] = pd.to_datetime(df['date']).dt.day.apply(lambda d: (d - 1) // 7 + 1)\n",
        "\n",
        "# Monthly seasonal index\n",
        "if 'month' in df.columns:\n",
        "    # Calculate monthly average per product\n",
        "    monthly_avg = df.groupby(['Product_Name', 'month'])['Quantity'].mean().reset_index()\n",
        "\n",
        "    # Calculate overall average per product\n",
        "    product_avg = monthly_avg.groupby('Product_Name')['Quantity'].mean().reset_index()\n",
        "\n",
        "    # Join to calculate seasonal index\n",
        "    monthly_avg = pd.merge(monthly_avg, product_avg, on='Product_Name', suffixes=('_month', '_product'))\n",
        "    monthly_avg['seasonal_index'] = monthly_avg['Quantity_month'] / monthly_avg['Quantity_product']\n",
        "\n",
        "    # Join seasonal index to main dataframe\n",
        "    df = pd.merge(df, monthly_avg[['Product_Name', 'month', 'seasonal_index']],\n",
        "                  on=['Product_Name', 'month'], how='left')\n",
        "\n",
        "# Fill NaN values for all time series features\n",
        "ts_columns = [col for col in df.columns if 'lag_' in col or 'moving_avg_' in col or\n",
        "              'trend' in col or 'ratio' in col or 'seasonal' in col or\n",
        "              'std' in col or 'flash' in col or 'holiday_yesterday' in col or\n",
        "              'payday_yesterday' in col]\n",
        "for col in ts_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "# Drop and Save\n",
        "created_times = df['Created_Time'].copy()\n",
        "product_names = df['Product_Name'].copy()\n",
        "variations = df['Variation'].copy() if 'Variation' in df.columns else None\n",
        "sizes = df['Size'].copy() if 'Size' in df.columns else None\n",
        "\n",
        "# Drop non-feature columns\n",
        "df.drop(columns=['Created_Time', 'date', 'year_month', 'year_week'], inplace=True, errors='ignore')\n",
        "df.drop(columns=['Product_Name', 'Variation', 'Size'], inplace=True, errors='ignore')\n",
        "\n",
        "after_ts_columns = df.columns.tolist()\n",
        "added_ts_features = [col for col in after_ts_columns if col not in after_time_columns]\n",
        "\n",
        "# Visualisasi\n",
        "if added_ts_features:\n",
        "    fig, ax = plt.subplots(figsize=(8, len(added_ts_features) * 0.3))\n",
        "    bars = ax.barh(added_ts_features, [1]*len(added_ts_features), edgecolor='black')\n",
        "\n",
        "    for bar in bars:\n",
        "        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)\n",
        "\n",
        "    ax.set_title('Fitur Baru dari Feature Engineering Time Series', fontsize=14, pad=10)\n",
        "    ax.set_xlabel('Fitur Baru')\n",
        "    ax.set_xlim(0, 1.2)\n",
        "    ax.set_xticks([])\n",
        "    ax.invert_yaxis()\n",
        "    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u92UkqOQnKa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Split"
      ],
      "metadata": {
        "id": "fpke6WR4yZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Validation-Test Split\n",
        "target_column = 'Quantity'\n",
        "columns_to_drop = ['year_month', 'date', 'flash_date_str']\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[target_column] + [col for col in columns_to_drop if col in df.columns])\n",
        "y = np.log1p(df[target_column])  # Log transform for better model performance\n",
        "\n",
        "# Time-based split\n",
        "train_size = int(0.8 * len(df))\n",
        "valid_size = int(0.1 * len(df))\n",
        "\n",
        "X_train = X.iloc[:train_size]\n",
        "y_train = y.iloc[:train_size]\n",
        "X_valid = X.iloc[train_size:train_size+valid_size]\n",
        "y_valid = y.iloc[train_size:train_size+valid_size]\n",
        "X_test = X.iloc[train_size+valid_size:]\n",
        "y_test = y.iloc[train_size+valid_size:]\n",
        "\n",
        "# Hitung jumlah sampel\n",
        "counts = [len(X_train), len(X_valid), len(X_test)]\n",
        "labels = ['Training', 'Validation', 'Testing']\n",
        "percentages = [count / (len(X_train) + len(X_valid) + len(X_test)) * 100 for count in counts]\n",
        "\n",
        "# Visualisasi\n",
        "print(\"Distribusi Jumlah Data:\")\n",
        "print(f\"Training\\t: {counts[0]} sampel ({percentages[0]:.1f}%)\")\n",
        "print(f\"Validation\\t: {counts[1]} sampel ({percentages[1]:.1f}%)\")\n",
        "print(f\"Testing\\t\\t: {counts[2]} sampel ({percentages[2]:.1f}%)\")\n",
        "print(f\"Total\\t\\t: {sum(counts)} sampel (100%)\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=labels, y=counts, hue=labels, palette='Set2', legend=False)\n",
        "\n",
        "for i, (count, pct) in enumerate(zip(counts, percentages)):\n",
        "    plt.text(i, count + 5, f'{pct:.1f}%', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.title('Distribusi Data: Train / Validation / Test')\n",
        "plt.ylabel('Jumlah Sampel')\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3mPDgkQ6nE6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "feature_selector = xgb.XGBRegressor(random_state=42)\n",
        "feature_selector.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_selector.feature_importances_\n",
        "})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "importance_mean = feature_importance['Importance'].mean()\n",
        "print(f\"Mean Importance Score: {importance_mean:.4f}\")\n",
        "\n",
        "selector = SelectFromModel(feature_selector, threshold=importance_mean, prefit=True)\n",
        "selected_features = X_train.columns[selector.get_support()]\n",
        "print(f\"Selected {len(selected_features)} features from total {X_train.shape[1]} initial features.\")\n",
        "\n",
        "# Use selected features for all data subsets\n",
        "X_train = X_train[selected_features]\n",
        "X_valid = X_valid[selected_features]\n",
        "X_test = X_test[selected_features]\n",
        "\n",
        "# Filter hanya fitur yang dipilih\n",
        "selected_feature_importance = feature_importance[feature_importance['Feature'].isin(selected_features)]\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=selected_feature_importance,\n",
        "    x='Importance',\n",
        "    y='Feature',\n",
        "    hue='Feature',\n",
        "    dodge=False,\n",
        "    palette='viridis',\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# Tambahkan nilai pada setiap batang\n",
        "for i, val in enumerate(selected_feature_importance['Importance']):\n",
        "    plt.text(val + 0.005, i, f\"{val:.3f}\", va='center')\n",
        "\n",
        "plt.axvline(importance_mean, color='red', linestyle='--', label='Mean Threshold')\n",
        "plt.title('Feature Importance dari Fitur yang Dipilih', fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Fitur', fontsize=12)\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… Terpilih {len(selected_features)} fitur dari total {X_train.shape[1]} fitur awal.\")"
      ],
      "metadata": {
        "id": "CrW52Gkayd_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}